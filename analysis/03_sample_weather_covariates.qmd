---
title: "03.2"
format: html
editor: visual
bibliography: ../references.bib
---

# Sampling Weather Covariates through Space-Time Regression Kriging

```{r}
#| label: packages
#| code-summary: "Loading packages"
#| message: false
#| warning: false

library(tidyverse)
packages <- c("sp", "sf", "gstat", "dbplyr", "meteo", "spacetime",
              "bestNormalize"," ggplot2", "gganimate", "patchwork", "tictoc", "glue")
walk(packages, require, character.only = T)
```

```{r}
set.seed(20240703) 
Sys.setenv(TZ='Europe/Stockholm')
readRenviron("../.env")
```

```{r}
load('../data/localisations/moose_random_steps.RData')
```

## 1. Retrieve Weather Data from Database

We retrieve weather stations and their data from an SMHI mock database created for this project. For details how to set up this database, view the `smhi/README.md` document. Make sure to insert credentials into the `.env` file at the repository root.

```{r}
con <- DBI::dbConnect(
  RPostgres::Postgres(), 
  dbname = Sys.getenv("DB_NAME"),
  host = Sys.getenv("DB_HOST"),
  user = Sys.getenv("DB_USERE"),
  password = Sys.getenv("DB_PASSWORD")
)

tbl_weather_data <- tbl(con, "weather_data")
tbl_weather_stations <- tbl(con, "weather_station")
```

Let's define a function that gets us all the weather data and their weather stations between two dates, for a given parameter (e.g. air temperature). We'll later split the data into `weather_data`, and `weather_stations` separately.

```{r}
get_weather_data <- function(start_date, end_date, parameter) {
  tbl_weather_data %>% 
    filter(
      parameter == parameter,
      quality == 'G',
      date_local >= start_date,
      date_local <= end_date
    ) %>%
    left_join(tbl_weather_stations, by=c("weather_station_id"="id")) %>%
    select(
      weather_station_id, 
      date_local,
      value, 
      height, 
      geom
    ) %>%
    rename(
      station_id = weather_station_id, 
      date = date_local, 
      value = value, 
      elevation = height,
    ) %>%
    # Retrieve all records that match the criteria from the database
    collect() %>%
    mutate(
      date = as.POSIXct(date, format="%Y-%m-%d %H:%M:%S", tz="Europe/Stockholm"),
      elevation = as.integer(elevation),  # Integer is enough precision for this use case
      month = month(date),  # Month number
      year = year(date),
      day = yday(date),  # Day number from 1 - 365
      geom = st_as_sfc(geom, crs=st_crs(3006))
    ) %>%
    # Use SF to process some geospatial variables
    st_as_sf(.) %>%
    # Geometry is provided in WKT, so we need to retrieve the coords manually
    mutate(x = st_coordinates(.)[,1], y = st_coordinates(.)[,2]) %>%
    # We'll also need latitude later to calculate GTT
    st_transform(., st_crs(4326)) %>%
    mutate(lat = st_coordinates(.)[,2]) %>%
    arrange(date) %>%
    st_drop_geometry()
}
```

## 2. Regression

Space-time regression is composed of two parts. The first part consists of predicting / catching the variation of our response variable through a regression model. For our regression model we'll follow methodologies adapted from [@spatio-t] and [@sekulic2020]. We'll use elevation derived from a Swedish DEM, and geometric temperature trend (GTT). To showcase the process this notebook goes through the regression and kriging step by step.

### 2.1 Data exploration

Let's first retrieve the data.

```{r}
weather_data <- data.frame()
for (year in unique(moose_random_steps$year)) {
  start_date <- as.POSIXct(glue("{year}-06-01 00:00:00"), tz="Europe/Stockholm")
  end_date <- as.POSIXct(glue("{year}-09-01 00:00:00"), tz="Europe/Stockholm")
  yearly_weather_data <- get_weather_data(start_date, end_date, "air_temperature")
  weather_data <- bind_rows(weather_data, yearly_weather_data)
}
n_stations <- length(unique(weather_data$station_id))
weather_data$year <- as.factor(weather_data$year)  # We'll need this later
```

```{r}
# Check duplicates, generally there are none, but for our data there is a single entry that is duplicated
duped <- which(duplicated(weather_data[,c("station_id", "x", "y","date")]))
print(duped)
weather_data <- weather_data[-duped,]
```

```{r}
plot1 <- ggplot(weather_data) +
  geom_boxplot(aes(y = value, fill = factor(year)), 
               alpha = 0.8, outlier.alpha = 0.008, 
               position = position_dodge(1)) +
  scale_fill_viridis_d(option = "magma", name = "Year") +
  theme_minimal() +
  labs(y = "Temperature (°C)", title = "Boxplots of Temperatures per Year") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank())

plot2 <- ggplot(weather_data %>% distinct(station_id, elevation)) +
  geom_histogram(aes(x = elevation), fill = "#FCA636FF", alpha = 0.8) +
  theme_minimal() +
  labs(x = "Elevation", y = "Amount of Weather Stations", 
       title = glue("Histogram of Weather Station Elevation (n={n_stations})"))

combined_plot <- plot1 + plot2 + plot_layout(nrow = 2, guides = "collect")

print(combined_plot)
```

The following plot shows the temperature readings animated over a couple of days. As you can see our data is not temporally regular for all weather stations. Some weather stations update every hour, some every 3 hours.

```{r}
start_date <- min(weather_data$date)
plot_data <- weather_data %>% filter(date <= start_date + days(3))
plot_data <- st_as_sf(plot_data, coords=c('x','y'))

p <- ggplot(plot_data) + 
  geom_sf(aes(color = value, group=station_id)) +
  scale_color_viridis_c(name = "Temperature (°C)", option = "magma") +
  labs(title = "Temperature over Time",
       subtitle = "Time: {frame_time}",
       x = "Long", y = "Lat") + 
  theme_minimal() + 
  transition_time(date) +
  enter_fade() + exit_fade()

gganimate::animate(p, renderer=gifski_renderer(), duration = 25, fps=30)
```

## 2.2 Modelling

GTT is the only co variate that is also temporally dynamic, and can be calculated given a latitude and time of day. There is a handy R function called `temp_geom` from the `meteo` library that does the work for us.

```{r}
weather_data$gtt <- temp_geom(weather_data$day, weather_data$lat, variable="mean")
```

```{r}
regression_model <- lm(value ~ elevation + gtt + year, data=weather_data)
summary(regression_model)
```

As we can see, a simple linear regression is not great at predicting temperature. Nevertheless capturing this variation will prove very useful as we continue with the spatial-temporal Kriging step. For that we'll be using the residuals of this model.

Lets double-check that our residuals are normally distributed:

```{r}
plot(regression_model, which=2)
```

We see temperature tends to decrease as we increase in elevation and GTT. We'll use this linear model to fit sample and experimental semi-variograms to residuals of its prediction.

```{r}
save(regression_model, file='../data/regression_model.RData')
```

We'll also need the prediction of the model (in combination with the kriged prediction) in order to derive the temperature estimation. It's easiest to predict each moose localisation now.

```{r}
build_pred_steps <- function(data, time_col, elev_col) {
  pred <- data
  pred$day <- yday(data[[time_col]])
  pred$year <- year(data[[time_col]])
  pred$year <- factor(pred$year, levels = levels(weather_data$year))
  pred$gtt <- temp_geom(pred$day, data$lat, variable = "mean")
  pred$elevation <- data[[elev_col]]
  return(pred)
}

# Build prediction datasets for start and end steps
pred_steps_start <- build_pred_steps(moose_random_steps, "t1_", "elevation_start")
pred_steps_end <- build_pred_steps(moose_random_steps, "t2_", "elevation_end")

# Use these datasets in the regression predictions
moose_random_steps$res_start <- unname(predict(regression_model, pred_steps_start))
moose_random_steps$res_end <- unname(predict(regression_model, pred_steps_end))

# Save the updated object
save(moose_random_steps, file = "../data/localisations/moose_random_steps.RData")

rm(pred_steps_start, pred_steps_end)
```

## 3. Spatial Temporal Regression Kriging

### 3.1 Making a test data set

Kriging using a data set of millions or rows would take a really long time. Therefore we split our data into week batches when predicting temperature at localisations. Here we'll walk through the process.

```{r}
start_date <- as.POSIXct("2005-07-17 21:00:00", tz="Europe/Stockholm")
end_date <- as.POSIXct("2005-07-25 03:00:00", tz="Europe/Stockholm")
weather_data <- get_weather_data(start_date, end_date, "air_temperature")
weather_data <- as.data.frame(weather_data)
```

### 3.1 Data preparation

We'll transform our data into a `STDF` (Space-Time Data-Frame) from the package `spacetime`. The package has a somewhat steep learning curve, but examples are provided in the [spacetime vignette](https://cran.r-project.org/web/packages/spacetime/vignettes/jss816.pdf). Another good resource is the [sftime demo](https://rdrr.io/cran/gstat/src/demo/sftime.R) of `gstat`.

```{r}
weather_data$year <- factor(weather_data$year, levels=regression_model$xlevels$year)
weather_data$gtt <- temp_geom(weather_data$day, weather_data$lat, variable="mean")
weather_data$res <- weather_data$value - unname(predict(regression_model, weather_data))
```

```{r}
stations <- weather_data %>% distinct(station_id, x, y)
obs <- weather_data %>% select(station_id, date, value, elevation, gtt, res)
```

```{r}
weather_stfdf <- meteo::meteo2STFDF(
  obs,
  stations,
  obs.staid.time = c("station_id", "date"),
  stations.staid.lon.lat = c("station_id", "x", "y"),
  crs=CRS('epsg:3006')
)
```

There are two weather stations in Sweden that share the exact location! Whilst we appreciate the enthusiasm this will severely break our modelling, let's remove one.

```{r}
zd <- zerodist(weather_stfdf@sp)
print(zd)
weather_stfdf <- weather_stfdf[-zd[,2], ]
```

A further step we could take is removing stations that are considered too close to each other. We can find the nearest station to each other station using `st_nearest_feature`. For every station that is \<100m from another, we'll remove one of the stations.

```{r}
weather_sp <- st_as_sf(weather_stfdf@sp)
nearest <- st_nearest_feature(weather_sp)
nearest_df <- data.frame(
  nearest_station_id = nearest,  
  distance = as.vector(st_distance(weather_sp, weather_sp[nearest,], by_element=TRUE))
) %>% arrange(distance)

head(nearest_df)
```

```{r}
nearest_df <- nearest_df %>% filter(distance < 100)
nearest_df <- nearest_df[duplicated(nearest_df$distance),]  # Keep one of the stations
nearest_df <- nearest_df[-nearest_df$nearest_station_id,]  # Remove the other
```

## 3.2 Fitting the semi-variogram

![Example of the parameters of a semivariogram](images/clipboard-1631625815.png){fig-align="center" width="350"}

First we fit the empirical variogram, what happens under the hood is a binning of data points based on spatial and temporal difference. The squared difference in their values is then averaged over those bins, giving us the variance over spatial and temporal distance.

```{r}
# Make sure this is hours, this will be the unit for the temporal variogram
diff_time <- as.integer(diff(index(weather_stfdf@time))[1]) 
tlags = 0:ceiling(10 / diff_time)  # estimate based on visual inspection, most our data is in 1 or 3 hour intervals
max(tlags)
```

```{r}
emperical_variogram <- variogramST(res~1, weather_stfdf, tlags=tlags)
```

```{r}
# coerce temporal difference unit to numeric (for compatibility)
emperical_variogram$timelag <- as.numeric(emperical_variogram$timelag)
plot(emperical_variogram)
plot(emperical_variogram, wireframe=T, scales=list(arrows=F))
```

Next we attempt to fit a sum-metric space-time variogram to the empirical variogram. We base our partial sill, range, and nugget values based on visual inspection of the empirical variogram. We definitely want a small nugget effect as a lot of our weather stations are very close to each other, which could lead to the screening effect.

```{r}
linear_st_anisotropy <- estiStAni(emperical_variogram, c(100000,400000), t.range=10)

sum_metric_model <- vgmST("sumMetric",
                          space=vgm(psill=6, "Sph", range=550000, nugget=1),
                          time= vgm(psill=30, "Sph", range=10, nugget=1), 
                          joint= vgm(psill=4, "Sph",  range=50000, nugget=1),
                          stAni=linear_st_anisotropy)

fit_sum_metric_model <- fit.StVariogram(
  emperical_variogram, 
  sum_metric_model, 
  lower = c(0,
            0.001,
            0,
            0.01,  # Nugget
            1), # Range
  fit.method = 8
)
```

```{r}
# set time unit for fitted model
attr(sum_metric_model, "temporal unit") <- "hours"
# check parameters fitted model
fit_sum_metric_model
# goodness of fit sum-metric model
attr(fit_sum_metric_model, "optim.output")$value
```

```{r}
plot(emperical_variogram, fit_sum_metric_model, wireframe=T, all=T, scales=list(arrows=F))
```

### 3.3 Predicting & Cross validation

TODO

### 3.4 Visual inspection of results through prediction maps

TODO

## 4 Sampling weather covariates at moose steps

Next we can predict some moose steps. As we'll be doing this on large scale with multi-threading, some helper functions have been written. The `03_functions.r` file contains functions that combine all of the essentials that have been carried out so far. It also contains a small helper function to get us started with moose steps. We can either predict the temperature at the start step or the end step, here we'll show an example with end steps.

```{r}
prepare_end_steps <- function(df) {
  df <- df %>%
    mutate(
      x    = x2_,
      y    = y2_,
      time = t2_,
      res  = res_end
    ) %>%
    arrange(time)
  return(df)
}

# The functions shapes the data for us and adds GTT
localisations <- prepare_end_steps(
  moose_random_steps %>% 
    filter(
      t2_ > start_date, 
      t2_ < start_date + days(1)
    )
)
localisations <- as.data.frame(localisations)

# krigeST wants a ST object, lets make one
moose_sf <- st_as_sf(localisations, coords = c("x", "y"), crs = CRS('epsg:3006'))
spatial_points <- as(moose_sf, "Spatial")
moose_st <- stConstruct(
  localisations, 
  space = c("x", "y"), 
  time  = "t2_", 
  spatial_points,
  crs = CRS('epsg:3006')
)

# Start Kriging with a timer
tic()
kriging_object <- krigeST(
  res~1, 
  weather_stfdf, 
  moose_st,
  sum_metric_model, 
  maxdist=100000,
  nmax=10, 
  computeVar = T
)
toc()
```

Next lets transform our prediction into a temperature estimate. We do this by combining the prediction of the linear model, to the prediction of the kriging. Remember that we initially applied a power transform, so we make sure to revert that.

```{r}
pred_kriging <- kriging_object@data$var1.pred
pred_combined <- pred_kriging + localisations$res_end
```

```{r}
print("Real Summary (Note: All of Sweden)")
summary(weather_data$value)
print("Reg. Kriged Summary")
summary(pred_combined)
print("Collar Temperature Summary")
summary(localisations$collar_temperature)
```

### 4.1. Using Parallelisation

Below is a separate piece of code that can be used to annotate all the steps in our `moose_random_steps` object with temperature data. The run-time of this will be rather significant, so take care not to run it accidentally.

```{r}
#| label: packages
#| code-summary: "Loading packages"
#| message: false
#| warning: false

library(tidyverse)
packages <- c("sp", "sf", "gstat", "dbplyr", "meteo", "spacetime")
walk(packages, require, character.only = T)

set.seed(20240703) 
Sys.setenv(TZ='Europe/Stockholm')
readRenviron("../.env")
```

```{r}
load('../data/localisations/moose_random_steps.RData')
load('../data/regression_model.RData')
```

```{r}
# We use all times from both start and end steps.
all_times <- c(moose_random_steps$t1_, moose_random_steps$t2_)
# Define week boundaries using floor_date. (week_start = 1 means Monday)
unique_week_starts <- sort(unique(floor_date(all_times, unit = "week", week_start = 1)))
buffer_hours <- 3

week_batches <- lapply(unique_week_starts, function(week_start) {
  main_start <- week_start
  main_end   <- week_start + days(7)
  # Define extended window: add buffer_hours before and after.
  extended_start <- main_start - hours(buffer_hours)
  extended_end   <- main_end + hours(buffer_hours)
  
  # Subset moose data: include any row if either t1_ falls within extended window.
  moose_subset <- moose_random_steps %>% 
    filter(t1_ >= main_start & t1_ < main_end) 
  if (nrow(moose_subset) == 0) return()
  
  # Pre-fetch weather data for this week’s extended window.
  weather_data <- get_weather_data(extended_start, extended_end, "air_temperature")
  # Add residuals to weather data
  weather_data$year <- factor(weather_data$year, levels=regression_model$xlevels$year)
  weather_data$gtt <- temp_geom(weather_data$day, weather_data$lat, variable="mean")
  weather_data$res <- weather_data$value - unname(predict(regression_model, weather_data))
  
  moose_subset <- moose_subset %>% select(
    animal_id, step_id_, x1_, x2_, y1_, y2_, t1_, t2_, year, res_start, res_end)
  weather_data <- weather_data %>% select(station_id, date, year, x, y, value, res)
  
  list(
    id        = as.character(week_start),
    moose_data     = moose_subset,
    weather_data   = weather_data
  )
})

save(week_batches, file='../data/week_batches.RData')
```
